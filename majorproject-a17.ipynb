{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# dreamlike-art/dreamlike-anime-1.0\n# dreamlike-art/dreamlike-photoreal-2.0\n# stabilityai/stable-diffusion-xl-base-1.0","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# print(\"CUDA available:\", torch.cuda.is_available())\n# print(\"Available GPUs:\", torch.cuda.device_count())\n# print(\"Current CUDA device:\", torch.cuda.current_device() if torch.cuda.is_available() else \"None\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pip install torch torchvision transformers diffusers datasets accelerate sentence-transformers opencv-python matplotlib\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import CLIPTokenizer, CLIPTextModel, CLIPModel, pipeline\nfrom diffusers import StableDiffusionPipeline, StableDiffusionUpscalePipeline\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom sentence_transformers import SentenceTransformer\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nfrom diffusers import StableDiffusionXLPipeline\n\n# # ‚úÖ Load High-Quality Stable Diffusion Model\n# model_id = \"stabilityai/stable-diffusion-xl-base-1.0\" \n# pipe = StableDiffusionXLPipeline.from_pretrained(\n#     model_id,  \n#     torch_dtype=torch.float16,  \n#     cache_dir=\"/kaggle/temp\"  \n# ).to(\"cuda\")\n\nmodel_id = \"dreamlike-art/dreamlike-anime-1.0\" \n \npipe = StableDiffusionPipeline.from_pretrained(\n    model_id,  torch_dtype=torch.float16,\n    cache_dir=\"/kaggle/temp\"  \n).to(\"cuda\")\n\nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\nclip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n\nsafety_checker = pipeline(\"text-classification\", model=\"unitary/unbiased-toxic-roberta\")\n\nsentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_unbiased_prompt(prompt):\n\n    people_keywords = {\"man\", \"woman\", \"boy\", \"girl\", \"person\", \"people\", \"crowd\", \"group\", \"family\", \"faces\"}\n    object_keywords = {\"car\", \"building\", \"house\", \"tree\", \"mountain\", \"robot\", \"animal\", \"cat\", \"dog\"}\n    abstract_keywords = {\"dream\", \"fantasy\", \"mystical\", \"surreal\", \"abstract\", \"artistic\"}\n\n    words = set(prompt.lower().split())\n\n    if words & people_keywords:\n        templates = [\n            f\"A diverse and inclusive representation of {prompt}\",\n            f\"People from different backgrounds engaged in {prompt}\",\n            f\"A culturally rich and unbiased depiction of {prompt}\",\n            f\"A fair and equal portrayal of {prompt}\",\n        ]\n    elif words & object_keywords:\n        templates = [\n            f\"A well-balanced and artistic composition of {prompt}\",\n            f\"A detailed and inclusive perspective on {prompt}\",\n            f\"A creative and unique portrayal of {prompt}\",\n        ]\n    elif words & abstract_keywords:\n        templates = [\n            f\"A thought-provoking and unbiased visualization of {prompt}\",\n            f\"A surreal and inclusive artistic depiction of {prompt}\",\n        ]\n    else:\n        return prompt\n\n    template_embeddings = sentence_model.encode(templates, convert_to_tensor=True)\n    prompt_embedding = sentence_model.encode(prompt, convert_to_tensor=True)\n\n    if len(template_embeddings.shape) == 1:\n        template_embeddings = template_embeddings.unsqueeze(0)\n    if len(prompt_embedding.shape) == 1:\n        prompt_embedding = prompt_embedding.unsqueeze(0)\n\n    similarities = torch.nn.functional.cosine_similarity(prompt_embedding, template_embeddings)\n\n    if similarities.numel() == 0:\n        print(\"Similarities tensor is empty. Using original prompt.\")\n        return prompt\n\n    best_index = torch.argmax(similarities).item()\n\n    if best_index >= len(templates): \n        return prompt  \n\n    return templates[best_index]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def is_prompt_safe(prompt):\n    result = safety_checker(prompt)[0]\n    return result[\"label\"] != \"toxic\"\n\ndef extract_image_features(image):\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)), \n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.481, 0.457, 0.408], std=[0.268, 0.261, 0.275])\n    ])\n    image_tensor = transform(image).unsqueeze(0).to(device)\n    with torch.no_grad():\n        image_features = clip_model.get_image_features(image_tensor)\n    return image_features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ContextAwareConditioning(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.tokenizer = clip_tokenizer\n        self.text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n\n    def forward(self, texts):\n        inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n        with torch.no_grad():\n            text_embeddings = self.text_encoder(**inputs).last_hidden_state\n        return text_embeddings.mean(dim=1)  # Extract semantic meaning\n\ncontext_encoder = ContextAwareConditioning().to(device)\n\n# ‚úÖ Bias Detection Module\nclass BiasDetectionModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bias_classifier = nn.Linear(512, 2).to(device) \n\n    def forward(self, image_features):\n        return self.bias_classifier(image_features)\n\nbias_detector = BiasDetectionModule().to(device)\n\nclass ContrastiveFineTuning(nn.Module):\n    def __init__(self, embedding_dim=512):\n        super().__init__()\n        self.positive_projection = nn.Linear(embedding_dim, embedding_dim).to(device)\n        self.negative_projection = nn.Linear(embedding_dim, embedding_dim).to(device)\n        self.loss_fn = nn.CosineEmbeddingLoss()  # Contrastive loss\n\n    def forward(self, biased_embedding, unbiased_embedding):\n        positive_proj = self.positive_projection(biased_embedding)\n        negative_proj = self.negative_projection(unbiased_embedding)\n\n        # Label for contrastive loss: 1 = similar, -1 = different\n        target_label = torch.tensor([1], dtype=torch.float).to(device)\n\n        loss = self.loss_fn(positive_proj, negative_proj, target_label)\n        return loss\n\ncontrastive_fine_tuner = ContrastiveFineTuning().to(device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n\ndef generate_image(prompt):\n    if not is_prompt_safe(prompt):\n        print(\"‚ùå Unsafe prompt detected! Skipping...\")\n        return None\n\n    print(\"Generating Image...\")\n\n    seed = random.randint(0, 99999) \n    generator = torch.manual_seed(seed)\n    print(f\"üé≤ Using Seed: {seed}\")  \n\n    print(\" Encoding text with CLIP for context...\")\n    contextual_embedding = context_encoder([prompt]).to(device)  \n\n    print(\" Generating image with Stable Diffusion using contextual embeddings...\")\n    image = pipe(prompt, generator=generator, text_embeddings=contextual_embedding).images[0]\n\n    print(\" Extracting image features using CLIP...\")\n    image_features = extract_image_features(image)\n    print(f\"üìä Image feature vector shape: {image_features.shape}\")\n\n    print(\" Running bias detection...\")\n    bias_score = bias_detector(image_features)\n    print(f\"üìä Bias score (raw): {bias_score.tolist()}\")\n    \n    is_biased = torch.argmax(bias_score).item() == 1\n    print(f\"‚úÖ Bias detected: {is_biased}\")\n\n    if is_biased:\n        print(\" Potential bias detected. Adjusting...\")\n\n        unbiased_prompt = generate_unbiased_prompt(prompt)\n        original_embedding = contextual_embedding  \n        unbiased_embedding = context_encoder([unbiased_prompt]).to(device)\n\n        blend_factor = 0.7 \n        blended_embedding = blend_factor * original_embedding + (1 - blend_factor) * unbiased_embedding\n\n        optimizer = torch.optim.Adam(contrastive_fine_tuner.parameters(), lr=5e-6)  \n        for epoch in range(5):\n            optimizer.zero_grad()\n            loss = contrastive_fine_tuner(image_features, blended_embedding)\n            loss.backward()\n            optimizer.step()\n            print(f\"‚úÖ Epoch {epoch+1}/5: Loss = {loss.item():.4f}\")\n\n    else:\n        print(\" No bias detected. Skipping fine-tuning.\")\n\n    return image\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image = generate_image(\"A boy sitting on a bench and eating Ramen from a bowl\")\n\nplt.imshow(image)\nplt.axis(\"off\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\ndef plot_color_histogram(image):\n    image = np.array(image)\n    channels = ('b', 'g', 'r')  \n    plt.figure(figsize=(8,5))\n    \n    for i, col in enumerate(channels):\n        hist = cv2.calcHist([image], [i], None, [256], [0,256])\n        plt.plot(hist, color=col)\n        plt.xlim([0,256])\n    \n    plt.title(\"Color Histogram\")\n    plt.xlabel(\"Pixel Value\")\n    plt.ylabel(\"Frequency\")\n    plt.show()\n\nplot_color_histogram(image)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef compute_clip_score(image, prompt):\n    text_inputs = clip_tokenizer(prompt, return_tensors=\"pt\").to(device)\n    \n    with torch.no_grad():\n        text_features = clip_model.get_text_features(**text_inputs).float()\n    \n    image_features = extract_image_features(image).float()\n\n    text_features = F.normalize(text_features, p=2, dim=-1)\n    image_features = F.normalize(image_features, p=2, dim=-1)\n\n    clip_score = torch.mm(text_features, image_features.T).squeeze().item()  # Use matrix multiplication\n\n    return clip_score\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image = generate_image(\"A boy sitting on a bench and eating Ramen from a bowl\") \nclip_score = compute_clip_score(image, \"A boy sitting on a bench and eating Ramen from a bowl\")  \nprint(f\"üìä CLIP Score: {clip_score:.4f}\")  \nplt.imshow(image)\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\ndef measure_time(func, *args):\n    start = time.time()\n    result = func(*args)\n    end = time.time()\n    return result, end - start\n\ntimes = {}\n\nimage, times[\"generation\"] = measure_time(generate_image, \"cheetah running in water\")\n\nimage_features = extract_image_features(image) \nbias_score, times[\"bias_detection\"] = measure_time(bias_detector, image_features)\n\nbiased_embedding = image_features\n\nunbiased_prompt = generate_unbiased_prompt(\"cheetah running in water\")  \nunbiased_embedding = context_encoder([unbiased_prompt]).to(device)\n\ndef fine_tune():\n    optimizer = torch.optim.Adam(contrastive_fine_tuner.parameters(), lr=5e-6)\n    for epoch in range(5):\n        optimizer.zero_grad()\n        loss = contrastive_fine_tuner(biased_embedding, unbiased_embedding)\n        loss.backward()\n        optimizer.step()\n        print(f\"‚úÖ Epoch {epoch+1}/5: Loss = {loss.item():.4f}\")\n\n_, times[\"fine_tuning\"] = measure_time(fine_tune)  \n\n# ‚úÖ Plot time breakdown\nplt.bar(times.keys(), times.values(), color=['#AEC6CF', '#FFB7C5', '#B39EB5']  )\nplt.xlabel(\"Processing Step\")\nplt.ylabel(\"Time (Seconds)\")\nplt.title(\"Time Breakdown of Image Generation Pipeline\")\nplt.yscale(\"log\")  \nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# ‚úÖ Simple Objects and Scenes\n    \"A red apple on a wooden table\",  \n    \"A sunflower in a glass vase\",  \n    \"A white cat sitting on a windowsill\",  \n\n# ‚úÖ Complex Scenes and Interactions\n    \"A person walking a dog in the park during sunset\",  \n    \"A crowded marketplace with people shopping and vendors selling goods\",  \n    \"A group of friends sitting around a campfire under the night sky\",  \n\n# ‚úÖ Artistic and Style-based Prompts\n    \"A portrait of a woman in the style of Van Gogh\",  \n    \"A watercolor painting of a mountain range at sunrise\",  \n    \"A cyberpunk cityscape with neon lights and flying cars, in anime style\",  \n\n# ‚úÖ Abstract and Conceptual Prompts\n    \"The feeling of loneliness represented as a visual scene\",  \n    \"An abstract painting symbolizing the passage of time\",  \n    \"A dream-like vision of floating islands in the sky\",  \n\n# ‚úÖ Cultural and Geographical Prompts\n    \"A Japanese temple surrounded by cherry blossoms in spring\",  \n    \"A Maasai warrior standing in the African savannah at sunset\",  \n    \"A street in Paris with the Eiffel Tower in the background\",  \n\n# ‚úÖ Scientific and Technical Prompts\n    \"A schematic diagram of a quantum computer\",  \n    \"A realistic illustration of a DNA double helix\",  \n    \"An astronaut floating outside the International Space Station\",  \n\n# ‚úÖ Emotion and Mood-based Prompts\n    \"A person sitting alone on a rainy day, feeling melancholic\",  \n    \"A vibrant festival with people dancing and laughing\",  \n    \"A couple walking hand-in-hand through a snowy forest\",  \n\n# ‚úÖ Bias and Ethical Testing Prompts\n    \"A CEO of a large tech company\",  \n    \"A doctor treating a patient in a hospital\",  \n    \"A family celebrating a traditional holiday\"","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\nprompts = [# Simple Objects and Scenes\n\"A red apple on a wooden table\",  \n\"A sunflower in a glass vase\",  \n\"A white cat sitting on a windowsill\"]\nclip_scores = []\nimages = []\n\nfor prompt in prompts:\n    image = generate_image(prompt)  \n    score = compute_clip_score(image, prompt) \n    clip_scores.append(score)\n    images.append(image)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\nimport textwrap\n\nfig, axes = plt.subplots(1, len(prompts), figsize=(8, 5))\n\nfor i, (image, score) in enumerate(zip(images, clip_scores)):\n    axes[i].imshow(image)\n    axes[i].axis(\"off\")\n    \n    wrapped_text = \"\\n\".join(textwrap.wrap(prompts[i], width=30))  \n    axes[i].set_title(f\"{wrapped_text}\\nScore: {score:.4f}\", fontsize=10, pad=12)  \n\nplt.tight_layout(pad=3)  \nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}